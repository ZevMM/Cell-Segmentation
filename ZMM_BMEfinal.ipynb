{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1694231,
          "sourceType": "datasetVersion",
          "datasetId": 1004164
        }
      ],
      "dockerImageVersionId": 30138,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "KG_ZMM_BMEfinal",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'singlecellsegmentation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1004164%2F1694231%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240311%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240311T175208Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8415ba72bbefaa47a50285285a65afdbfe8c09da4706c8da234f74da4d46c0302aa7cacf9d48c337a292257d0d4eb348f8b15b6c451c78fe15104b650ef616a04f22f2a0c94a5a56ac83ea00bfdda169575b9be686fe1ace616fa5ba0a6a2dc12a5508ef9ca25be5f24ef67181856b8f6576944b270ff3542746581695514969a2852037517403fccb778e3e0d53b9da8b5dded351536719657075d8c645296302de5306ee2e730f09f1cb3a1f6ebd7912dd589fb607b633a41efc83c96301f315e8194c415b63ee0861f95bf70cec5a68223ce0ea4557801e6996e3f469d2899a255f5e3d443ac22c0568ddcddb770f348b5353de80dfbfae8b505e28eac615'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XmsFoFIypLmS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:11.161722Z",
          "iopub.execute_input": "2023-12-14T22:31:11.162314Z",
          "iopub.status.idle": "2023-12-14T22:31:13.868682Z",
          "shell.execute_reply.started": "2023-12-14T22:31:11.162216Z",
          "shell.execute_reply": "2023-12-14T22:31:13.867953Z"
        },
        "trusted": true,
        "id": "rG1KyZ7TpLmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "print(\"GPU available:\", cuda)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:13.87062Z",
          "iopub.execute_input": "2023-12-14T22:31:13.871245Z",
          "iopub.status.idle": "2023-12-14T22:31:13.940576Z",
          "shell.execute_reply.started": "2023-12-14T22:31:13.871198Z",
          "shell.execute_reply": "2023-12-14T22:31:13.939644Z"
        },
        "trusted": true,
        "id": "5Co3HqrIpLmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1102)\n",
        "np.random.seed(1102)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:13.942308Z",
          "iopub.execute_input": "2023-12-14T22:31:13.942651Z",
          "iopub.status.idle": "2023-12-14T22:31:13.952991Z",
          "shell.execute_reply.started": "2023-12-14T22:31:13.942602Z",
          "shell.execute_reply": "2023-12-14T22:31:13.952255Z"
        },
        "trusted": true,
        "id": "wqkbtOOvpLmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/input/singlecellsegmentation/SingleCellSegmentation/train/image/image_*.png | wc -l"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:13.954915Z",
          "iopub.execute_input": "2023-12-14T22:31:13.955205Z",
          "iopub.status.idle": "2023-12-14T22:31:29.256344Z",
          "shell.execute_reply.started": "2023-12-14T22:31:13.955172Z",
          "shell.execute_reply": "2023-12-14T22:31:29.255409Z"
        },
        "trusted": true,
        "id": "0GI8PHxopLmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/input/singlecellsegmentation/SingleCellSegmentation/valid/image/image_*.png | wc -l"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:29.257859Z",
          "iopub.execute_input": "2023-12-14T22:31:29.258161Z",
          "iopub.status.idle": "2023-12-14T22:31:32.917415Z",
          "shell.execute_reply.started": "2023-12-14T22:31:29.2581Z",
          "shell.execute_reply": "2023-12-14T22:31:32.916615Z"
        },
        "trusted": true,
        "id": "-4z9zYV6pLmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/input/singlecellsegmentation/SingleCellSegmentation/test/image/image_*.png | wc -l"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:32.91878Z",
          "iopub.execute_input": "2023-12-14T22:31:32.919047Z",
          "iopub.status.idle": "2023-12-14T22:31:39.24108Z",
          "shell.execute_reply.started": "2023-12-14T22:31:32.919014Z",
          "shell.execute_reply": "2023-12-14T22:31:39.240347Z"
        },
        "trusted": true,
        "id": "FDjQLG8xpLmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root_folder = '../input/singlecellsegmentation/SingleCellSegmentation/'\n",
        "class BasicDataset(TensorDataset):\n",
        "    # This function takes folder name ('train', 'valid', 'test') as input and creates an instance of BasicDataset according to that fodler.\n",
        "    # Also if you'dd like to have less number of samples (for evaluation purposes), you may set the `n_sample` with an integer.\n",
        "    def __init__(self, folder, n_sample=None):\n",
        "        self.folder = os.path.join(data_root_folder, folder)\n",
        "        self.imgs_dir = os.path.join(self.folder, 'image')\n",
        "        self.masks_dir = os.path.join(self.folder, 'mask')\n",
        "\n",
        "        self.imgs_file = sorted(glob.glob(os.path.join(self.imgs_dir, '*.png')))\n",
        "        self.masks_file = sorted(glob.glob(os.path.join(self.masks_dir, '*.png')))\n",
        "\n",
        "        assert len(self.imgs_file) == len(self.masks_file), 'There are some missing images or masks in {0}'.format(folder)\n",
        "\n",
        "        # If n_sample is not None (It has been set by the user)\n",
        "        if not n_sample or n_sample > len(self.imgs_file):\n",
        "            n_sample = len(self.imgs_file)\n",
        "\n",
        "        self.n_sample = n_sample\n",
        "        self.ids = list([i+1 for i in range(n_sample)])\n",
        "\n",
        "    # This function returns the lenght of the dataset (AKA number of samples in that set)\n",
        "    def __len__(self):\n",
        "        return self.n_sample\n",
        "\n",
        "\n",
        "    # This function takes an index (i) which is between 0 to `len(BasicDataset)` (The return of the previous function), then returns RGB image,\n",
        "    # mask (Binary), and the index of the file name (Which we will use for visualization). The preprocessing step is also implemented in this function.\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        img = cv2.imread(os.path.join(self.imgs_dir, 'image_{0:04d}.png'.format(idx)), cv2.IMREAD_COLOR)\n",
        "        mask = cv2.imread(os.path.join(self.masks_dir, 'mask_{0:04d}.png'.format(idx)), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        #Resize all images from 512 to 256 (H and W)\n",
        "        img = cv2.resize(img, (256,256))\n",
        "        mask = cv2.resize(mask, (256,256))\n",
        "\n",
        "        # Scale between 0 to 1\n",
        "        img = np.array(img) / 255.0\n",
        "        mask = np.array(mask) / 255.0\n",
        "\n",
        "        # Make sure that the mask are binary (0 or 1)\n",
        "        mask[mask <= 0.5] = 0.0\n",
        "        mask[mask > 0.5] = 1.0\n",
        "\n",
        "        # Add an axis to the mask array so that it is in [channel, width, height] format.\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "\n",
        "        # HWC to CHW\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
        "            'mask': torch.from_numpy(mask).type(torch.FloatTensor),\n",
        "            'img_id': idx\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:39.242475Z",
          "iopub.execute_input": "2023-12-14T22:31:39.242723Z",
          "iopub.status.idle": "2023-12-14T22:31:39.259127Z",
          "shell.execute_reply.started": "2023-12-14T22:31:39.242691Z",
          "shell.execute_reply": "2023-12-14T22:31:39.258244Z"
        },
        "trusted": true,
        "id": "OPxjH0CHpLmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, validation, and test dataset instances\n",
        "train_dataset = BasicDataset('train')\n",
        "valid_dataset = BasicDataset('valid')\n",
        "test_dataset = BasicDataset('test')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Data split distribution')\n",
        "plt.bar(0, len(train_dataset), label='Train', color='paleturquoise')\n",
        "plt.bar(1, len(valid_dataset), label='Validation', color='lavender')\n",
        "plt.bar(2, len(test_dataset), label='Test', color='mistyrose')\n",
        "plt.ylabel('Number of samples')\n",
        "plt.xticks([0,1,2],['Train', 'Validation', 'Test'])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:39.260892Z",
          "iopub.execute_input": "2023-12-14T22:31:39.261202Z",
          "iopub.status.idle": "2023-12-14T22:31:41.425028Z",
          "shell.execute_reply.started": "2023-12-14T22:31:39.261164Z",
          "shell.execute_reply": "2023-12-14T22:31:41.424294Z"
        },
        "trusted": true,
        "id": "Nkv0Gj1dpLmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = np.random.randint(0, len(train_dataset))\n",
        "data = train_dataset.__getitem__(sample)\n",
        "x = data['image']\n",
        "y = data['mask']\n",
        "idx = data['img_id']\n",
        "\n",
        "print(f'x shape is {x.shape}')\n",
        "print(f'y shape is {y.shape}')\n",
        "\n",
        "plt.figure(figsize=(12, 8), dpi=100)\n",
        "plt.suptitle(f'Sample {idx:04d}')\n",
        "img = np.transpose(x, (1,2,0))\n",
        "mask = y[0]\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Image')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Mask')\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:41.426252Z",
          "iopub.execute_input": "2023-12-14T22:31:41.42678Z",
          "iopub.status.idle": "2023-12-14T22:31:42.208158Z",
          "shell.execute_reply.started": "2023-12-14T22:31:41.426749Z",
          "shell.execute_reply": "2023-12-14T22:31:42.207301Z"
        },
        "trusted": true,
        "id": "xgL8WymxpLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create train, validation, and test dataset instances to reduce the number of samples and expedite the training process.\n",
        "train_dataset = BasicDataset('train', n_sample=1000)\n",
        "valid_dataset = BasicDataset('valid', n_sample=200)\n",
        "test_dataset = BasicDataset('test', n_sample=200)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=4, num_workers=2, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:42.211366Z",
          "iopub.execute_input": "2023-12-14T22:31:42.211646Z",
          "iopub.status.idle": "2023-12-14T22:31:42.313471Z",
          "shell.execute_reply.started": "2023-12-14T22:31:42.211613Z",
          "shell.execute_reply": "2023-12-14T22:31:42.312713Z"
        },
        "trusted": true,
        "id": "x6gl0v0mpLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## Double Convolution\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "######################################## Maxpooling followed by Double Convolution\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "######################################## Upsampling followed by Double Convolution\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up_conv = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
        "        )\n",
        "        self.conv = DoubleConv(out_channels * 2, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up_conv(x1)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "######################################## Output layer (1x1 Convolution followed by SoftMax activation)\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv_sigmoid = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_sigmoid(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:42.315006Z",
          "iopub.execute_input": "2023-12-14T22:31:42.315517Z",
          "iopub.status.idle": "2023-12-14T22:31:42.334785Z",
          "shell.execute_reply.started": "2023-12-14T22:31:42.315478Z",
          "shell.execute_reply": "2023-12-14T22:31:42.333978Z"
        },
        "trusted": true,
        "id": "3YH5tBihpLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, name, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.name = name\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inputL = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outputL = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inputL(x)\n",
        "\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        b = self.down4(x4)\n",
        "\n",
        "        x = self.up1(b, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        x = self.outputL(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:42.336281Z",
          "iopub.execute_input": "2023-12-14T22:31:42.336556Z",
          "iopub.status.idle": "2023-12-14T22:31:42.350436Z",
          "shell.execute_reply.started": "2023-12-14T22:31:42.33652Z",
          "shell.execute_reply": "2023-12-14T22:31:42.349523Z"
        },
        "trusted": true,
        "id": "WLg8POV5pLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_UNet = UNet('MyUNet', 3, 1)\n",
        "my_UNet.cuda()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:42.351681Z",
          "iopub.execute_input": "2023-12-14T22:31:42.352391Z",
          "iopub.status.idle": "2023-12-14T22:31:45.813982Z",
          "shell.execute_reply.started": "2023-12-14T22:31:42.352333Z",
          "shell.execute_reply": "2023-12-14T22:31:45.813229Z"
        },
        "trusted": true,
        "id": "hI4bf_72pLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first batch\n",
        "for batch in test_dataloader:\n",
        "    sample_batch = batch\n",
        "    break\n",
        "\n",
        "# Generat network prediction\n",
        "with torch.no_grad():\n",
        "    y_pred = my_UNet(sample_batch['image'].cuda())\n",
        "\n",
        "# Print the shapes of the images, masks, predicted masks\n",
        "print('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape,\n",
        "                                                                                                                       sample_batch['mask'].shape,\n",
        "                                                                                                                       y_pred.shape\n",
        "                                                                                                                      ))\n",
        "\n",
        "# Conver Pytorch tensor to numpy array then reverse the preprocessing steps\n",
        "img = (sample_batch['image'][0].numpy().transpose(1,2,0) * 255).astype('uint8')\n",
        "msk = (sample_batch['mask'][0][0,:,:].numpy() * 255).astype('uint8')\n",
        "\n",
        "# Exctract the relative prediction mask and threshold the probablities (>0.5)\n",
        "pred_msk = (y_pred.cpu().numpy()[0][0,:,:] * 255).astype('uint8')\n",
        "pred_msk_binary = ((y_pred.cpu().numpy()[0][0,:,:] > 0.5) * 255).astype('uint8')\n",
        "\n",
        "# Take the image id for display\n",
        "img_id = sample_batch['img_id'][0]\n",
        "\n",
        "# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\n",
        "plt.figure(figsize=(24,18))\n",
        "plt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n",
        "\n",
        "plt.subplot(2,4,1)\n",
        "plt.title('Input Image', fontsize=15)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,2)\n",
        "plt.title('Ground Truth', fontsize=15)\n",
        "plt.imshow(msk, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,3)\n",
        "plt.title('Non-trained Network Prediction Output \\n(probability [0, 1])', fontsize=15)\n",
        "plt.imshow(pred_msk, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,4)\n",
        "plt.title('Non-trained Thresholdded Binary Prediction (threshold > 0.5)', fontsize=15)\n",
        "plt.imshow(pred_msk_binary, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "input_overlayed_GT = img.copy()\n",
        "input_overlayed_GT[msk == 255, :] = [0, 255, 0]\n",
        "plt.subplot(2,4,5)\n",
        "plt.title('Input Image overlayed with Ground Truth', fontsize=15)\n",
        "plt.imshow(input_overlayed_GT)\n",
        "plt.axis('off')\n",
        "\n",
        "input_overlayed_Pred = img.copy()\n",
        "input_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\n",
        "plt.subplot(2,4,6)\n",
        "plt.title('Input Image overlayed with Prediction', fontsize=15)\n",
        "plt.imshow(input_overlayed_Pred)\n",
        "plt.axis('off')\n",
        "\n",
        "GT_overlayed_prediction = np.zeros_like(img)\n",
        "GT_overlayed_prediction[msk == 255, 1] = 255\n",
        "GT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\n",
        "plt.subplot(2,4,7)\n",
        "plt.title('Ground Truth overlayed with Prediction', fontsize=15)\n",
        "plt.imshow(GT_overlayed_prediction)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:45.815486Z",
          "iopub.execute_input": "2023-12-14T22:31:45.815685Z",
          "iopub.status.idle": "2023-12-14T22:31:52.916388Z",
          "shell.execute_reply.started": "2023-12-14T22:31:45.815661Z",
          "shell.execute_reply": "2023-12-14T22:31:52.915215Z"
        },
        "trusted": true,
        "id": "DM-edn37pLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:52.917904Z",
          "iopub.execute_input": "2023-12-14T22:31:52.918149Z",
          "iopub.status.idle": "2023-12-14T22:31:52.925298Z",
          "shell.execute_reply.started": "2023-12-14T22:31:52.918101Z",
          "shell.execute_reply": "2023-12-14T22:31:52.924442Z"
        },
        "trusted": true,
        "id": "3PURCWhFpLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(my_UNet.parameters(), lr=0.001)\n",
        "loss_function = DiceLoss()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:52.926528Z",
          "iopub.execute_input": "2023-12-14T22:31:52.926809Z",
          "iopub.status.idle": "2023-12-14T22:31:52.94142Z",
          "shell.execute_reply.started": "2023-12-14T22:31:52.926767Z",
          "shell.execute_reply": "2023-12-14T22:31:52.940618Z"
        },
        "trusted": true,
        "id": "iWZtCTDBpLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Training loop"
      ],
      "metadata": {
        "id": "o46h5s2tpLmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that computes the DICE score for binary segmentation\n",
        "def dice_coeff_binary(y_pred, y_true):\n",
        "        \"\"\"Values must be only zero or one.\"\"\"\n",
        "        eps = 0.0001\n",
        "        inter = torch.dot(y_pred.view(-1), y_true.view(-1))\n",
        "        union = torch.sum(y_pred) + torch.sum(y_true)\n",
        "        return ((2 * inter.float() + eps) / (union.float() + eps)).cpu().numpy()\n",
        "\n",
        "\n",
        "# The training function\n",
        "def train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function):\n",
        "\n",
        "    if not os.path.isdir('{0}'.format(net.name)):\n",
        "        os.mkdir('{0}'.format(net.name))\n",
        "\n",
        "    n_train = len(train_dataloader)\n",
        "    n_valid = len(valid_dataloader)\n",
        "\n",
        "    train_loss = list()\n",
        "    valid_loss = list()\n",
        "    train_dice = list()\n",
        "    valid_dice = list()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        ################################################################################################################################\n",
        "        ########################################################### Training ###########################################################\n",
        "        ################################################################################################################################\n",
        "        net.train()\n",
        "        train_batch_loss = list()\n",
        "        train_batch_dice = list()\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Load a batch and pass it to the GPU\n",
        "            imgs = batch['image'].cuda()\n",
        "            true_masks = batch['mask'].cuda()\n",
        "\n",
        "            # Produce the estimated mask using current weights\n",
        "            y_pred = net(imgs)\n",
        "\n",
        "            # Compute the loss for this batch and append it to the epoch loss\n",
        "            loss = loss_function(y_pred, true_masks)\n",
        "            batch_loss = loss.item()\n",
        "            train_batch_loss.append(batch_loss)\n",
        "\n",
        "            # Make the thresholded mask to compute the DICE score\n",
        "            pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n",
        "\n",
        "            # Compute the DICE score for this batch and append it to the epoch dice\n",
        "            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n",
        "            train_batch_dice.append(batch_dice_score)\n",
        "\n",
        "\n",
        "            # Reset gradient values\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute the backward losses\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print the progress\n",
        "            print(f'EPOCH {epoch + 1}/{epochs} - Training Batch {i+1}/{n_train} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n",
        "\n",
        "        average_training_loss = np.array(train_batch_loss).mean()\n",
        "        average_training_dice = np.array(train_batch_dice).mean()\n",
        "        train_loss.append(average_training_loss)\n",
        "        train_dice.append(average_training_dice)\n",
        "\n",
        "        ################################################################################################################################\n",
        "        ########################################################## Validation ##########################################################\n",
        "        ################################################################################################################################\n",
        "\n",
        "        net.eval()\n",
        "        valid_batch_loss = list()\n",
        "        valid_batch_dice = list()\n",
        "\n",
        "        # This part is almost the same as training with the difference that we will set all layers to evaluation mode (effects some layers such as BN and Dropout) and also\n",
        "        # we don't need to calculate the gradient since we are only evaluating current state of the model. This will speed up the process and cause it to consume less memory.\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(valid_dataloader):\n",
        "\n",
        "                # Load a batch and pass it to the GPU\n",
        "                imgs = batch['image'].cuda()\n",
        "                true_masks = batch['mask'].cuda()\n",
        "\n",
        "                # Produce the estimated mask using current weights\n",
        "                y_pred = net(imgs)\n",
        "\n",
        "                # Compute the loss for this batch and append it to the epoch loss\n",
        "                loss = loss_function(y_pred, true_masks)\n",
        "                batch_loss = loss.item()\n",
        "                valid_batch_loss.append(batch_loss)\n",
        "\n",
        "                # Make the thresholded mask to compute the DICE score\n",
        "                pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n",
        "\n",
        "                # Compute the DICE score for this batch and append it to the epoch dice\n",
        "                batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n",
        "                valid_batch_dice.append(batch_dice_score)\n",
        "\n",
        "                # Print the progress\n",
        "                print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n",
        "\n",
        "        average_validation_loss = np.array(valid_batch_loss).mean()\n",
        "        average_validation_dice = np.array(valid_batch_dice).mean()\n",
        "        valid_loss.append(average_validation_loss)\n",
        "        valid_dice.append(average_validation_dice)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}/{epochs} - Training Loss: {average_training_loss}, Training DICE score: {average_training_dice}, Validation Loss: {average_validation_loss}, Validation DICE score: {average_validation_dice}')\n",
        "\n",
        "        ################################################################################################################################\n",
        "        ###################################################### Saveing Checkpoints #####################################################\n",
        "        ################################################################################################################################\n",
        "        torch.save(net.state_dict(), f'{net.name}/epoch_{epoch+1:03}.pth')\n",
        "\n",
        "    return train_loss, train_dice, valid_loss, valid_dice"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:52.942783Z",
          "iopub.execute_input": "2023-12-14T22:31:52.943051Z",
          "iopub.status.idle": "2023-12-14T22:31:52.971909Z",
          "shell.execute_reply.started": "2023-12-14T22:31:52.943019Z",
          "shell.execute_reply": "2023-12-14T22:31:52.97116Z"
        },
        "trusted": true,
        "id": "14qwkgIipLma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 51\n",
        "train_loss, train_dice, valid_loss, valid_dice = train_net(my_UNet, EPOCHS, train_dataloader, valid_dataloader, optimizer, loss_function)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T22:31:52.973493Z",
          "iopub.execute_input": "2023-12-14T22:31:52.973945Z",
          "iopub.status.idle": "2023-12-14T23:06:56.71381Z",
          "shell.execute_reply.started": "2023-12-14T22:31:52.973893Z",
          "shell.execute_reply": "2023-12-14T23:06:56.713085Z"
        },
        "trusted": true,
        "id": "2ETGYhgjpLma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.suptitle('Learning Curve', fontsize=18)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(EPOCHS)+1, train_loss, '-o', label='Training Loss', color = 'paleturquoise')\n",
        "plt.plot(np.arange(EPOCHS)+1, valid_loss, '-o', label='Validation Loss', color = 'mistyrose')\n",
        "plt.xticks(np.arange(EPOCHS)+1)\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(EPOCHS)+1, train_dice, '-o', label='Training DICE score', color = 'paleturquoise')\n",
        "plt.plot(np.arange(EPOCHS)+1, valid_dice, '-o', label='Validation DICE score', color = 'mistyrose')\n",
        "plt.xticks(np.arange(EPOCHS)+1)\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('DICE score', fontsize=15)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:06:56.715416Z",
          "iopub.execute_input": "2023-12-14T23:06:56.715679Z",
          "iopub.status.idle": "2023-12-14T23:06:58.092865Z",
          "shell.execute_reply.started": "2023-12-14T23:06:56.715645Z",
          "shell.execute_reply": "2023-12-14T23:06:58.092099Z"
        },
        "trusted": true,
        "id": "H_llwObipLmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = np.argmax(valid_dice) + 1 # The plus one is because the epochs starts at 1.\n",
        "\n",
        "print(f'Best epoch is epoch{best_epoch}')\n",
        "\n",
        "state_dict = torch.load(f'./MyUNet/epoch_{best_epoch:03}.pth')\n",
        "\n",
        "my_UNet.load_state_dict(state_dict)\n",
        "my_UNet.cuda()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:06:58.094222Z",
          "iopub.execute_input": "2023-12-14T23:06:58.094486Z",
          "iopub.status.idle": "2023-12-14T23:06:58.199564Z",
          "shell.execute_reply.started": "2023-12-14T23:06:58.094454Z",
          "shell.execute_reply": "2023-12-14T23:06:58.198861Z"
        },
        "trusted": true,
        "id": "qTuIoa97pLmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first batch\n",
        "for batch in test_dataloader:\n",
        "    sample_batch = batch\n",
        "    break\n",
        "\n",
        "# Generat network prediction\n",
        "with torch.no_grad():\n",
        "    y_pred = my_UNet(sample_batch['image'].cuda())\n",
        "\n",
        "# Print the shapes of the images, masks, predicted masks\n",
        "print('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape,\n",
        "                                                                                                                       sample_batch['mask'].shape,\n",
        "                                                                                                                       y_pred.shape\n",
        "                                                                                                                      ))\n",
        "\n",
        "# Conver Pytorch tensor to numpy array then reverse the preprocessing steps\n",
        "img = (sample_batch['image'][0].numpy().transpose(1,2,0) * 255).astype('uint8')\n",
        "msk = (sample_batch['mask'][0][0,:,:].numpy() * 255).astype('uint8')\n",
        "\n",
        "# Exctract the relative prediction mask and threshold the probablities (>0.5)\n",
        "pred_msk = (y_pred.cpu().numpy()[0][0,:,:] * 255).astype('uint8')\n",
        "pred_msk_binary = ((y_pred.cpu().numpy()[0][0,:,:] > 0.5) * 255).astype('uint8')\n",
        "\n",
        "# Take the image id for display\n",
        "img_id = sample_batch['img_id'][0]\n",
        "\n",
        "# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\n",
        "plt.figure(figsize=(24,18))\n",
        "plt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n",
        "\n",
        "plt.subplot(2,4,1)\n",
        "plt.title('Input Image', fontsize=15)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,2)\n",
        "plt.title('Ground Truth', fontsize=15)\n",
        "plt.imshow(msk, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,3)\n",
        "plt.title('Final Network Prediction Output \\n(probability [0, 1])', fontsize=15)\n",
        "plt.imshow(pred_msk, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2,4,4)\n",
        "plt.title('Final Thresholdded Binary Prediction (threshold > 0.5)', fontsize=15)\n",
        "plt.imshow(pred_msk_binary, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "input_overlayed_GT = img.copy()\n",
        "input_overlayed_GT[msk == 255, :] = [0, 255, 0]\n",
        "plt.subplot(2,4,5)\n",
        "plt.title('Input Image overlayed with Ground Truth', fontsize=15)\n",
        "plt.imshow(input_overlayed_GT)\n",
        "plt.axis('off')\n",
        "\n",
        "input_overlayed_Pred = img.copy()\n",
        "input_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\n",
        "plt.subplot(2,4,6)\n",
        "plt.title('Input Image overlayed with Prediction', fontsize=15)\n",
        "plt.imshow(input_overlayed_Pred)\n",
        "plt.axis('off')\n",
        "\n",
        "GT_overlayed_prediction = np.zeros_like(img)\n",
        "GT_overlayed_prediction[msk == 255, 1] = 255\n",
        "GT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\n",
        "plt.subplot(2,4,7)\n",
        "plt.title('Ground Truth overlayed with Prediction', fontsize=15)\n",
        "plt.imshow(GT_overlayed_prediction)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:06:58.200657Z",
          "iopub.execute_input": "2023-12-14T23:06:58.200866Z",
          "iopub.status.idle": "2023-12-14T23:06:59.672142Z",
          "shell.execute_reply.started": "2023-12-14T23:06:58.20084Z",
          "shell.execute_reply": "2023-12-14T23:06:59.671132Z"
        },
        "trusted": true,
        "id": "hltRhjLMpLmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_net(net, test_dataloader, loss_function):\n",
        "    # Create the pred_mask folder\n",
        "    if not os.path.isdir('/kaggle/working/pred_mask'):\n",
        "        os.mkdir('/kaggle/working/pred_mask')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    n_test = len(test_dataloader)\n",
        "    test_batch_loss = list()\n",
        "    test_batch_dice = list()\n",
        "    test_batch_accuray = list()\n",
        "    test_batch_CM = list()\n",
        "\n",
        "    # This part is almost the same as the validation loop in `train_net` function.\n",
        "    # The difference is that we will calculate the accuracy and confusion matrix per each batch and save the predicted images.\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_dataloader):\n",
        "\n",
        "            # Load a batch and pass it to the GPU\n",
        "            imgs = batch['image'].cuda()\n",
        "            true_masks = batch['mask'].cuda()\n",
        "            img_ids = batch['img_id'].numpy().astype('int')\n",
        "\n",
        "            # Produce the estimated mask using current weights\n",
        "            y_pred = net(imgs)\n",
        "\n",
        "            # Compute the loss for this batch and append it to the epoch loss\n",
        "            loss = loss_function(y_pred, true_masks)\n",
        "            batch_loss = loss.item()\n",
        "            test_batch_loss.append(batch_loss)\n",
        "\n",
        "            # Make the thresholded mask to compute the DICE score\n",
        "            pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n",
        "\n",
        "            # Compute the DICE score for this batch and append it to the epoch dice\n",
        "            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n",
        "            test_batch_dice.append(batch_dice_score)\n",
        "\n",
        "            # Save the predicted masks\n",
        "            for idx, pred_msk in enumerate(pred_binary):\n",
        "                cv2.imwrite(f'/kaggle/working/pred_mask/pred_mask_{img_ids[idx]:04}.png', np.expand_dims((pred_msk[0].cpu().numpy() * 255).astype('uint8'), axis=-1))\n",
        "\n",
        "            # Vectorize the true mask and predicted mask for this batch\n",
        "            vectorize_true_masks = true_masks.view(-1).cpu().numpy()\n",
        "            vectorize_pred_masks = pred_binary.view(-1).cpu().numpy()\n",
        "\n",
        "            # Compute the accuracy for this batch and append to the overall list\n",
        "            batch_accuracy = accuracy_score(vectorize_true_masks, vectorize_pred_masks)\n",
        "            test_batch_accuray.append(batch_accuracy)\n",
        "\n",
        "            # Compute the normalized confusion matrix for this batch and append to the overall list\n",
        "            batch_CM = confusion_matrix(vectorize_true_masks, vectorize_pred_masks, normalize='true', labels=[0, 1])\n",
        "            test_batch_CM.append(batch_CM)\n",
        "\n",
        "            # Print the progress\n",
        "            print(f'Test Batch {i+1}/{n_test} - Loss: {batch_loss}, DICE score: {batch_dice_score}, Accuracy: {batch_accuracy}', end='\\r')\n",
        "\n",
        "    test_loss = np.array(test_batch_loss).mean()\n",
        "    test_dice = np.array(test_batch_dice).mean()\n",
        "    test_accuracy = np.array(test_batch_accuray).mean()\n",
        "    test_CM = np.array(test_batch_CM).mean(axis=0)\n",
        "\n",
        "    return test_loss, test_dice, test_accuracy, test_CM"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:06:59.673671Z",
          "iopub.execute_input": "2023-12-14T23:06:59.673942Z",
          "iopub.status.idle": "2023-12-14T23:06:59.693212Z",
          "shell.execute_reply.started": "2023-12-14T23:06:59.67391Z",
          "shell.execute_reply": "2023-12-14T23:06:59.692365Z"
        },
        "trusted": true,
        "id": "KqMrIajXpLmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_dice, test_accuracy, test_CM = test_net(my_UNet, test_dataloader, loss_function)\n",
        "\n",
        "print(f'Test Loss: {test_loss}, Test DICE score: {test_dice}, Test overall accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:06:59.694518Z",
          "iopub.execute_input": "2023-12-14T23:06:59.694797Z",
          "iopub.status.idle": "2023-12-14T23:08:43.835036Z",
          "shell.execute_reply.started": "2023-12-14T23:06:59.694764Z",
          "shell.execute_reply": "2023-12-14T23:08:43.834158Z"
        },
        "trusted": true,
        "id": "P1BsG7-QpLmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(test_CM, index = ['Background', 'Cell'],\n",
        "                     columns = ['Background', 'Cell'])\n",
        "plt.figure(figsize = (12,10))\n",
        "plt.title('Confusion matrix')\n",
        "sns.heatmap(df_cm, annot = True, fmt='.2%', annot_kws = {\"size\": 15}, cmap = 'PuBuGn')\n",
        "plt.ylim([0, 2]);\n",
        "plt.ylabel('True labels');\n",
        "plt.xlabel('predicted labels');"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T23:08:43.836519Z",
          "iopub.execute_input": "2023-12-14T23:08:43.836773Z",
          "iopub.status.idle": "2023-12-14T23:08:44.158887Z",
          "shell.execute_reply.started": "2023-12-14T23:08:43.836741Z",
          "shell.execute_reply": "2023-12-14T23:08:44.158159Z"
        },
        "trusted": true,
        "id": "flSLsdTfpLmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
